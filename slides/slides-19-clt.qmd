---
title: "Central Limit Theorem"
date: "April 9, 2025"
title-slide-attributes:
    data-background-image: "figs/bikeshare-plots.png"
    data-background-size: contain
    data-background-opacity: "0.2"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
    scrollable: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: false
---

## Recap

```{r echo = F, message= F}
knitr::opts_chunk$set(echo = F, warning = F, messabundance = F)
library(tidyverse)
library(openintro)
library(readr)
library(kableExtra)
plot_theme <- theme(text = element_text(size = 24))
source("../stat201_fns.R")
```

-   Normal distribution: symmetric, bell-shaped curve that is described by mean $\mu$ and standard deviation $\sigma$

    -   Common model used to describe behavior of continuous variables

-   Use area under the Normal curve to obtain probabilities

-   68-95-99.7 rule

-   z-score standardizes observations to allow for easier comparison: $z = \frac{x- \mu}{\sigma}$

## Warm-up

-   Let $Z \sim N(0,1)$. If the 10th percentile of $Z$ is `r round(qnorm(0.1), 2)`, what is the 90th percentile?

-   Let $X \sim N(0,2)$. If the 10th percentile of $X$ is `r round(qnorm(0.1, 0, 2), 2)`, what is the 90th percentile? Or can you not say without code?

-   Let $Y \sim N(2,1)$. If the 10th percentile of $Y$ is `r round(qnorm(0.1, 2, 1), 2)`, what is the 90th percentile? Or can you not say without code?

## Where we're going

-   We are going to learn one of the BIGGEST theorems in Statistics

-   Uses the Normal distribution, and will be immensely helpful for inference tasks of confidence intervals and hypothesis testing

# Central Limit Theorem

## Central Limit Theorem (CLT)

-   Assume that you have a **sufficiently large** sample of $n$ **independent** values $x_{1},\ldots, x_{n}$ from a population with mean $\mu$ and standard deviation $\sigma$.

-   Then the distribution of sample means is approximately Normal:

    $$
    \bar{X} \overset{\cdot}{\sim} N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)
    $$

    -   That is, the *sampling distribution* of the sample mean is approximately normal with mean $\mu$ and standard error $\sigma/\sqrt{n}$

## CLT assumptions

1.  Independent samples:

    -   Usually achieved by random sampling

2.  Normality condition:

    -   If the data $x_{1},\ldots, x_{n}$ are known to be Normal and independent, then the distribution of $\bar{X}$ is ***exactly*** Normal

    <!-- -->

    -   If data are not known to be Normal, then check:

        -   If $n$ is small $(n < 30)$: if there are no clear outliers, we assume data are approximately normal

        -   If $n$ is larger $(30 \leq n < ?)$: if there are no particularly extreme outliers, we assume data are approximately normal

## Flow chart

## Normality condition

::: discuss
Do you believe the large sample size/normality condition is satisfied in the following two samples?
:::

::::: columns
::: {.column width="65%"}
![](figs/16-normality.png){width="600"}
:::

::: {.column width="35%"}
-   Sample 1: small $n < 30$. But histogram and boxplot reveals no clear outliers, so I would say normality condition is met.

-   Sample 2: larger $n \geq 30$. Even though $n$ is larger, there is a particularly extreme outlier, so I would say normality condition is not met.
:::
:::::

## CLT again

Let's see it again: If the assumptions of independence and Normality condition apply, then

$$\bar{X} \overset{\cdot}{\sim} N\left(\mu, \frac{\sigma}{n} \right)$$
where $\mu$ and $\sigma$ are the population mean and standard deviation, and $\bar{X}$ is the sample mean obtained from a sample of size $n$.

::: {.discuss}
- What does the $\frac{\sigma}{n}$ represent?
- For fixed $\sigma$, how does the sampling distribution change as $n$ increases?
:::
## Height example

```{r}
mu <- round(mean(nba_heights$h_in), 2)
sigma <- round(sd(nba_heights$h_in), 2)
```

The average height of all NBA players in the 2008-9 season is `r mu` inches, with a population standard deviation of `r sigma` inches. We randomly sampled $20$ of these players and recorded their heights, as shown below.

```{r}
set.seed(16)
n <- 20
x <- sample(nba_heights$h_in, n)
ggplot(data.frame(x), aes(x = x)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Player height (in)") +
  theme_minimal() +
  theme(text = element_text(size = 22))
```

:::: fragment
::: discuss
What is the sampling distribution of the sample mean heights? Do we know it exactly?
:::
::::

## Height example: solution

We don't know if the data are Normal. But:

1.  Independence? Yes: we have independent samples!
2.  Normality condition? Yes: even though we have small sample size, the histogram of the data looks approximately Normal (no clear outliers).

-   So CLT applies! By CLT: $\bar{X} \overset{\cdot}{\sim} N\left(`r mu`, \frac{`r sigma`}{\sqrt{`r n`}}\right)$

:::::: columns
::: {.column width="50%"}
-   If data instead looked like the following, I would say normality condition is violated:
:::

:::: {.column width="50%"}
::: fragment
```{r}
set.seed(2)
height_sort <- nba_heights |>
  arrange(desc(h_in)) |>
  slice(1:100) |>
  sample_n(n-1) |>
  pull(h_in)
x <- c(height_sort, min(nba_heights$h_in))
ggplot(data.frame(x), aes(x = x)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Player height (in)") +
  theme_minimal() +
  theme(text = element_text(size = 30))
```
:::
::::
::::::

## Bank example

Customers are standing in line at a bank.

-   Let $X_{i}$ represent the service time for customer $i$.

-   Suppose that the average service time for all customers is 5 minutes, with a standard deviation of 6 minutes.

::: discuss
-   Assume that a bank currently has 36 customers in it, and all customers are independent of each other. What is the probability that the average service time of all these customers is less than 4 minutes?
:::

## Bank example: solution

-   We want $\text{Pr}(\bar{X} < 4)$

-   Conditions for CLT met: independence (random sample) and sufficiently large sample size $(n=36)$.

    -   So by CLT, $\bar{X} \overset{\cdot}{\sim}N(5, \frac{6}{\sqrt{36}}) = N(5, 1)$

-   Using 68-95-99.7 rule, probability that the average service time of all these customers is less than 4 minutes is about $1 - (0.34 + 0.5) = 0.16$

    -   `pnorm(4, 5, 1)` = `r round(pnorm(4,5,1), 3)`

# CLT for proportions

Remember: a proportion can be viewed as a mean! So the CLT will apply to proportions as well!

## **CLT for sample proportions**

Suppose we have some true population proportion $p$. If we take a sample of size $n$ from the population, then the CLT tells us that **sampling distribution of** $\hat{p}$ is approximately Normal if we have:

1.  Independence

2.  "Success-failure" condition: $np \geq 10$ *and* $n(1-p) \geq 10$

::: fragment
If these two conditions hold, then by CLT:

$$
\hat{p} \overset{\cdot}{\sim} N\left(p, \sqrt{\frac{p(1-p)}{n}}\right)
$$
:::

::: discuss
-   Why is the condition called "success-failure"?
-   Are you comfortable with using a Normal distribution to approximate the sampling distribution of $\hat{p}$?
:::

## M&M's example

Mars, Inc. is the company that makes M&M's. In 2008, Mars changed their color distribution to have 13% red candies.

:::: fragment
::: discuss
Let $\hat{p}$ represent the proportion of red M&M's in a random sample of $n$ M&M's. What is the sampling distribution of $\hat{p}$ if we take a random sample of sizes:

-   $n = 100$

-   $n = 10$
:::
::::

## M&M's example: solution

1.  Independence? Yes, due to the random sample.
2.  Success-failure? Depends...

:::: {.columns}
::: {.column width="60%"}
-   **If** $n= 100$**:**

    -   $np = 100(0.13) = 13 \geq 10$

    -   $n(1-p) = 100(0.87) = 87 \geq 10$

-   So CLT applies!

::: {.fragment}
$$
\begin{align*}
\hat{p} &\overset{\cdot}{\sim} N\left(0.13, \sqrt{\frac{0.13(1-0.13)}{100}}\right) \\
&= N(0.13, 0.034 )
\end{align*}
$$
:::
:::
::: {.column width="40%"}
::: fragment

```{r fig.height=8}
p <- 0.13
n <- 100
ggplot() +
   geom_function(fun = dnorm, args = list(mean = p, sd = sqrt(p*(1-p)/n)) )+
  theme_minimal() +
  scale_x_continuous(limits = c(-0.2, 1.1), n.breaks = 6) +
  theme(axis.text.y = element_blank(), axis.title.y = element_blank()) +
  labs(x = expression(hat(p)), title = "Theoretical sampling distribution", subtitle = "By CLT") +
  theme(text = element_text(size = 30))
```
:::
:::
::::

## M&M's example: solution (cont.)

-   **If** $n = 10$**:**
    -   $np = 10(0.13) = 1.3 < 10$
-   Success-failure condition not met. **Cannot use CLT.**

:::: {.columns}
::: {.column width="50%"}
- If we incorrectly applied CLT, we *might* think 
$$\hat{p} \overset{\cdot}{\sim} N\left(0.13, \sqrt{\frac{0.13(1-0.13)}{10}}\right)$$
- What does this distribution look like?
:::
::: {.column width="50%"}
::: {.fragment}
::: {.discuss}
Why is this scary??
:::
```{r fig.height = 7}
n <- 10
ggplot() +
   geom_function(fun = dnorm, args = list(mean = p, sd = sqrt(p*(1-p)/n)) )+
  theme_minimal() +
  scale_x_continuous(limits = c(-0.5, 1.1), n.breaks = 6) +
  theme(axis.text.y = element_blank(), axis.title.y = element_blank()) +
  labs(x = expression(hat(p)), title = "Incorrect theoretical sampling distribution") +
  theme(text = element_text(size = 30))
```
:::
:::
::::






## M&M's example (cont.)

The following histograms display sampling distributions for $\hat{p}$ = proportion of red candies in random samples of size $n = \{10, 50, 100, 200\}$:

::: fragment
```{r fig.height=5}
# red, orange, yellow, green, blue, brown
mnm_props <- c(0.13, 0.20, 0.14, 0.16, 0.24, 0.13)
get_red <- function(n){
  mean(sample(c("red", "orange", "yellow", "green", "blue", "brown"), size = n, replace = T, prob = mnm_props) == "red")
}
B <- 1000
set.seed(190)
n1 <- 50; prop1 <- replicate(B, get_red(n = n1))
n2 <- 100; prop2 <- replicate(B, get_red(n = n2))
n3 <- 10; prop3 <- replicate(B, get_red(n = n3))
n4 <- 200; prop4 <- replicate(B, get_red(n = n4))
data.frame(prop1, prop2, prop3, prop4) |>
  pivot_longer(cols = 1:4, values_to = "p_hat", names_to = "n") |>
  mutate(n = case_when(n == "prop1" ~ n1,
                       n == "prop2" ~ n2,
                       n == "prop3" ~ n3,
                       n == "prop4" ~ n4),
         n_lab = paste("n =", n),
         n_lab = fct_reorder(n_lab, n)) |>
  ggplot(aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02, col = "white")+
  facet_wrap(~n_lab, ncol = 2, scales = "free_y") +
  labs(x = expression(hat(p)), y = NULL) +
  scale_y_continuous(breaks = NULL)+
  # theme_minimal() +
  theme(text = element_text(size = 20))
```
:::

## Why is CLT so important?

1.  Allows statisticians safely assume that the mean's sampling distribution is approximately Normal. The Normal distribution has nice properties and is easy to work with.

2.  Can be applied to both continuous and discrete numeric data!

3.  Does not depend on the underlying distribution of the data.

-   For many of these reasons, we can use the CLT for inference!

# Confidence Intervals via CLT

## Mathematical CIs

-   Rather than using simulation techniques (i.e. bootstrap) to obtain the sampling distribution, the CLT gives us the sampling distribution of a mean "for free"

    -   (assuming conditions are met)

-   Formula for a (symmetric) $\gamma \times 100\%$ confidence interval:

    $$
    \text{point estimate} \pm \underbrace{\text{critical value} \times \text{SE}}_{\text{Margin of Error}}
    $$

    1.  **point estimate**: the "best guess" statistic from our observed data (e.g. $\hat{p}$ and $\bar{x}$)

    2.  **SE**: standard error of the statistic

    3.  **critical value**: percentile that guarantees the $\gamma\times 100$. This will vary depending on your data/assumptions

## Towards a CI for a single proportion

Suppose that I have a sample of $n$ binary (0/1) values. I want a $\gamma \times 100\%$ confidence interval for the probability of success $p$ using the sample.

::: fragment
[**If**]{.underline} assumptions of CLT for sample proportions hold, then we know

$$
\hat{p} \overset{\cdot}{\sim} N\left(p, \sqrt{\frac{p(1-p)}{n}}\right)
$$
:::

-   We can use/manipulate this result to obtain a confidence interval for the unknown $p$!

-   How do we know if success-failure condition holds without knowing $p$?

    -   Let's use our best guess: $\hat{p}$

    -   Success-failure condition *for inference*: $n\hat{p}$ and $n(1-\hat{p})$ both $\geq 10$

## Towards a CI for a single proportion (cont.)

1.  Point estimate: observed $\hat{p}$ from our sample

2.  Standard error: $\sqrt{p(1-p)/n}$

    -   But we still don't have $p$!

    -   Instead, use the following approximation for CI:

::: fragment
$$\text{SE}(\hat{p}) \approx \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$
:::

3.  Critical value: to obtain the middle $\gamma \times 100\%$ part, we use the $(1-\gamma)/2$ and $(1+\gamma)/2$ percentiles of the $N(0,1)$ distribution

    -   $z_{(1-\gamma)/2}^{*}$ (lower bound) and $z_{(1+\gamma)/2}^{*}$ (upper bound)

    -   Note: $z_{(1+\gamma)/2}^{*} = - z_{(1-\gamma)/2}^{*}$

## CI for single proportion

So the formula for a (symmetric) $\gamma\times 100\%$ CI for $p$ is:

::: fragment
$$
\hat{p} \pm z_{(1+\gamma)/2}^{*}\times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$where the critical value is obtained from $N(0,1)$ distribution
:::

::: {.fragment style="color: maroon"}
Come take STAT 311 to see why this is our CI!
:::

## Example

A poll of 100 randomly sampled registered voters in a town was conducted, asking voters if they support legalized marijuana. It was found that 60% of respondents were in support.

::: discuss
What is the population parameter? What is the point estimate/statistic?
:::

::: {.fragment style="color: maroon"}
Find a (symmetric) 90% confidence interval for the true proportion of town residents in favor of legalized marijuana.
:::

-   ::: discuss
    Conditions for CLT met?
    :::

    -   Independence: random sample

    -   Success-failure condition: $n\hat{p} =100(0.6) = 60 \geq 10$ and $n(1-\hat{p}) = 100(0.4) = 40 \geq 10$

## Example (cont.)

::: {.fragment style="color: maroon"}
Find a (symmetric) 90% confidence interval for the true proportion of town residents in favor of legalized marijuana.
:::

::: fragment
Gathering components for CI:
:::

1.  Point estimate: $\hat{p}$ = 0.6

2.  Standard error: $\text{SE}(\hat{p}) \approx \sqrt{\frac{0.6(0.4)}{100}} \approx 0.049$

3.  ::: discuss
    Critical value: what percentiles do we want?
    :::

    -   $z_{0.95}^{*} =$ `qnorm(0.95, mean = 0, sd = 1)` $\approx 1.645$

::: fragment
So our 90% confidence interval for $p$ is:

$$
0.6 \pm 1.645(0.049) = (0.519, 0.681)
$$
:::

:::: fragment
::: discuss
Interpret the confidence interval in context!
:::
::::

## Comprehension questions

-   What is the main takeaway of the CLT?

-   What are the assumptions of the CLT?

-   How do we construct a $\gamma \times 100\%$ confidence interval using a mathematical model?
